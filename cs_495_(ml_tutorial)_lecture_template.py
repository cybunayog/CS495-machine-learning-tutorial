# -*- coding: utf-8 -*-
"""CS 495 (ML Tutorial) - Lecture Template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19n9l7ZGxQbEfUW11MVGGTlciNk3br5Sl

# CS 495 (ML) - Lecture Demo
Welcome to the APU CS495 Machine Learning lecture demo. In this notebook, we will demonstrate how to read in a basic dataset, perform some data cleansing operations and generate several machine learning models to make predictions based on the given dataset.

# 1 - Load & Initialize Data
## Upload Dataset and Supporting Files into CoLab Environment
In order to create and execute our ML models, we will need access to a dataset and, in our case, a supporting Python file (for drawing nice confusion matrices.

We can upload supporting files to this "session", but we would have to upload them each time we opened up CoLab. Instead, it's easier to connect your Google Drive so that they are "permanently" mounted.
"""

# Mount your Google Drive to access files
from google.colab import drive
drive.mount('/content/drive')

# Copy the dataset and supporting Python file(s) to the local directory
!cp '/content/drive/MyDrive/Colab Notebooks/confusion_matrix_pretty_print.py' .
!cp '/content/drive/MyDrive/Colab Notebooks/iris.data' .

"""## Import Libraries & Load Data
First, we must initialize the environment and import data from the CSV file into a Pandas dataframe:
"""

# Imports
import pandas as pd
import numpy as np
import math
from IPython import display

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn import preprocessing # Label encoding & Data scaling

# Import method from file (I made chagnes so that it didn't have depricated methods)
# File/package sourced from: http://github.com/wcipriana/pretty-print-confusion-matrix
from confusion_matrix_pretty_print import pretty_plot_confusion_matrix

# Test that Pandas is installed and imported
pd.__version__
# Load dataset
df_iris = pd.read_csv("iris.data")

"""## Display Numerical Data
Next, we clean up the data a bit and print basic stats on the number-based columns:
"""

# Output basic dataset stats
df_iris.describe()

"""## Display Non-Numerical Data
Next, we print basic (and less useful) stats on the non-number columns and print the unique label names:
"""

# Print stats on non-numeric features
# df_iris.describe(include=['O'])
# Print unique labels
cm_labels = df_iris["Class"].unique()
print(cm_labels)

from google.colab import drive
drive.mount('/content/drive')

"""# 2 - Pre-process Data
## Encode String Labels
Convert the labels (in String format) to integers, which are more easily processed by ML algorithms:

"""

# Generate label encoder object
label_enc = preprocessing.LabelEncoder()
# Convert Strings to ints and print unique ints
df_iris["Class"] = label_enc.fit_transform(df_iris["Class"])
df_iris["Class"].unique()
df_iris.describe()

"""## Filter Data
Here, we may choose to filter out data based on the values of certain features or labels:
"""

# NOTHING for now (KEEP all data where the class label is greater or equal to 0 - which is ALL DATA in this case)
# df_iris = df_iris[df_iris.Petal_Length >= 2]
# df_iris.describe()
# df_iris.head(n = 10)

"""## Randomize Data
Randomize data and print first few rows for confirmation:
"""

# Randomize ordering of all samples
df_iris = df_iris.reindex(np.random.permutation(df_iris.index))
df_iris.head(n = 10)
# Save randomized data to file and print the first few to show randomization
df_iris.to_csv("iris_RANDOMIZED.data", index = False)

"""## Select Columns for Features & Labels
The following methods pre-process the data by extracting the relevant features and targets into separate dataframes:
"""

# Takes in a Pandas DataFrame taht contains a raw dataset and returns a
# Pandas DataFrame that contains only the selected features used for a model
def get_features_dataframe(df_input):
    
    # Create a new/blank DataFrame
    df_selected = pd.DataFrame()

    # Grab any features already available
    df_selected["Sepal_Length"] = df_input["Sepal_Length"]
    df_selected["Sepal_Width"] = df_input["Sepal_Width"]
    df_selected["Petal_Length"] = df_input["Petal_Length"]
    df_selected["Petal_Width"] = df_input["Petal_Width"]
    
    # Make a copy of the selected features
    df_processed = df_selected.copy()
        
    # Create any desired synthetic features
    df_processed["Sepal_Area"] = df_input["Sepal_Length"] * df_input["Sepal_Width"]
        
    # Return the selected features (both pre-existing and synthetic)
    return df_processed
    
# Takes in a Pandas DataFrame taht contains a raw dataset and returns a
# Pandas DataFrame that contains only the selected target(s) used for a model
def get_targets_dataframe(df_input):
    
    # Create a new/blank DataFrame
    df_selected = pd.DataFrame()
        
    # Grab any features already available
    df_selected["iris_label"] = df_input["Class"]
        
    # Make a copy of the selected features
    df_processed = df_selected.copy()
        
    # Create any desired synthetic features
    # Do nothing
        
    # Return the selected features (both pre-existing and synthetic)
    return df_processed

"""## Seperate Data into Training & Testing Sets
Select the:
- percentage of data to be used for classic test/validation split training
- number of folds for cross-validation
"""

# Percentage (0-1.0 corresponds to 0% to 100%) of dataset
percent_training_data = 0.8       # CHANGE ME
percent_validation_data = 1 - percent_training_data
num_cv_folds = 5                  # CHANGE ME

"""Now separate the data into training and validation sets by setting the percentage of data to be used for training:"""

# Choose the first (percent_training_data)% examples for training
num_total_examples = len(df_iris)
num_training_examples = math.ceil(num_total_examples * percent_training_data)
num_validation_examples = num_total_examples - num_training_examples

# Get all examples (useful later on...)
df_features_all = get_features_dataframe(df_iris.head(num_total_examples))
df_targets_all = get_targets_dataframe(df_iris.head(num_total_examples))

# Choose the first (percent_training_data)% for training examples
df_features_training = get_features_dataframe(df_iris.head(num_training_examples))
df_targets_training = get_targets_dataframe(df_iris.head(num_training_examples))

# Choose the last (1-percent_training_data)% for validation examples
df_features_validation = get_features_dataframe(df_iris.tail(num_validation_examples))
df_targets_validation = get_targets_dataframe(df_iris.tail(num_validation_examples))

"""## Display Summary of Training/Testing Data (SANITY CHECK)
Print out basic stats of the training and validation data for both the features and targets/labels. Means (averages) between the training and validation features/targets should be close if data was properly randomized:
"""

# Print summary of data split
print("{} total examples used:".format(num_total_examples))
print("\t{:2f}% ({} examples used for training)".format(num_training_examples/num_total_examples * 100, num_training_examples))
print("\t{:2f}% ({} examples used for validation)".format(num_validation_examples/num_total_examples * 100, num_validation_examples))

# Display summary of features data
print("\nTraining features summary:")
display.display(df_features_training.describe())
print("\nValidation features summary:")
display.display(df_features_validation.describe())

# Display summary of labels/targets data
print("\nTraining targets/labels summary:")
display.display(df_targets_training.describe())
print("\nValidation targets/labels summary:")
display.display(df_targets_validation.describe())

"""## Standardize Data & Display (SANITY CHECK)
Standardize all feature data so that it looks like Gaussian distribution with 0 MEAN and UNIT 1 variation (standard deviation). Display results for sanity check:
"""

# Create scalar from training examples and normalize both training and validation examples
scaler = preprocessing.StandardScaler().fit(df_features_training)
df_features_training_normalized = pd.DataFrame(scaler.transform(df_features_training))
df_features_validation_normalized = pd.DataFrame(scaler.transform(df_features_validation))

# Display summary of feature data
print("\nTraining Features Summary:")
display.display(df_features_training_normalized.describe())
print("\nValidation Features Summary:")
display.display(df_features_validation_normalized.describe())
# For more tips on scaling data in SCIKIT-LEARN:
# https://scikit-learn.org/stable/modules/preprocessing.html

# NOTE: Validation data won't have the same closeness to 0 MEAN and 1 STD b/c the scaler was created on the training data

"""# 3 - Generate Machine Learning Models & Make Predictions
## Variable Initialization
The following code creates multiple arrays for the purposes of code simplicity:
"""

# Variables to hold multiple models (and associated data)
list_model_names = ["Logistic Regression (LR)", "Support Vector Classifier (SVC)",  "Linear Support Vector Classifier (LSVC)",  "K-nearest Neighbors (KNN)"]
list_models = []
list_model_predictions = []
list_model_cm = []

"""## Train Data
The following code fits several classifiers to the training data:
"""

print("Model Parameters:", end="\n\n\t")

# Train/fit Logistic Regression Model
from sklearn import linear_model
logistic = linear_model.LogisticRegression(solver="lbfgs", max_iter=1000, multi_class="multinomial")
logistic.fit(df_features_training_normalized, df_targets_training.to_numpy().ravel())
list_models.append(logistic)
print(logistic, end="\n\n\t")

# Train/fit Support Vector Classification Model
from sklearn import svm
svc = svm.SVC(kernel="linear", class_weight="balanced")
svc.fit(df_features_training_normalized, df_targets_training.to_numpy().ravel())
list_models.append(svc)
print(svc, end="\n\n\t")

# Train/fit Linear Support Vector Classification Model
lsvc = svm.LinearSVC(penalty='l2', loss='squared_hinge')
lsvc.fit(df_features_training_normalized, df_targets_training.to_numpy().ravel())
list_models.append(lsvc)
print(lsvc, end="\n\n\t")

# Train/fit K-nearest Neighbors Model
from sklearn import neighbors
knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')
knn.fit(df_features_training_normalized, df_targets_training.to_numpy().ravel())
list_models.append(knn)
print(knn, end="\n\n\t")

"""## Make Predictions
The following code makes predictions and prints the raw prediction arrays:
"""

# Predict validation examples and print
for i in range(len(list_model_names)):  
  prediction = list_models[i].predict(df_features_validation_normalized)
  list_model_predictions.append(prediction)
  print("{} Predictions:".format(list_model_names[i]))

  # Print prediction and validation labels
  print("\t{}<== PREDICTION".format(prediction))
  print("\t{}<== ACTUAL".format(df_targets_validation.to_numpy().ravel()))

"""# 4 - Formatted Results
## Generate Stats
Generate confusion matrices and labels to display:
"""

# Generate confusion matrices
for i in range(len(list_model_names)):
  cm = confusion_matrix(df_targets_validation, list_model_predictions[i])
  list_model_cm.append(cm)

"""## Display Basic Summary
The following code prints basic results:
"""

# Print correctness of each model
for i in range(len(list_model_names)):
    print("{} Prediction Accuracy: ".format(list_model_names[i]))
    
    # Print results for classic split of test and validation data
    print("\tResults for classic {:.0f}/{:.0f} (training/testing) split:".format(percent_training_data * 100, percent_validation_data * 100))
    overall_score = list_models[i].score(df_features_validation_normalized, df_targets_validation)
    print("\t\tOverall: {:.2f}%".format(overall_score * 100))

    # Print out scores for individual classes
    for j in range(len(cm_labels)):
      print("\t\t\t{:s}: {:.2f}%".format(cm_labels[j], list_model_cm[i][j][j] / sum(list_model_cm[i][j]) * 100))

    # Print results for cross-validation
    cv_results  = cross_val_score(list_models[i], df_features_all, df_targets_all.to_numpy().ravel(), cv=num_cv_folds)
    print("\tResults for classic {:d}-fold cross-validation:".format(num_cv_folds))
    print("\t\tOverall: {:.2f}%\n".format(np.mean(cv_results) * 100))

"""## Display Confusion Matrices
The following code generates and dispalys the confusion matrix of the previous predictions:
"""

# Print confusion matrices    
for i in range(len(list_model_names)):
  title = "{} Confusion Matrix".format(list_model_names[i])
  df_cm = pd.DataFrame(list_model_cm[i], index=cm_labels, columns=cm_labels)
  pretty_plot_confusion_matrix(df_cm, cmap="PuRd", pred_val_axis="X", title=title)